# ============================================================================
# NFX-Identity Configuration Example
# ============================================================================
# 
# This is a comprehensive example configuration file for NFX-Identity services.
# Copy this file to inputs/{service}/configuration/{env}.toml and modify 
# according to your service and environment.
# 
# Configuration structure matches modules/*/config/types.go
# Each service may have different optional sections (token, email, storage)
#
# IMPORTANT: All values use environment variable placeholders in the format:
#   "${VAR_NAME}"
# 
# These placeholders will be automatically replaced with values from the .env
# file at runtime. If a variable is missing, the application will exit.
#
# See .example.env for all available environment variables.
#
# ============================================================================

# ============================================================================
# Server Configuration
# ============================================================================
[server]
    # Service name (e.g., "Access Service", "Auth Service")
    name = "<Service> Service"
    
    # Server host address (use "0.0.0.0" to listen on all interfaces)
    host = "0.0.0.0"
    
    # HTTP REST API port (from .env: HTTP_PORT)
    http_port = "${HTTP_PORT}"
    
    # gRPC service port (from .env: GRPC_PORT_<MODULE>)
    # Replace <MODULE> with: ACCESS, AUTH, AUDIT, CLIENTS, DIRECTORY, IMAGE, SYSTEM, TENANTS
    grpc_port = "${GRPC_PORT_<MODULE>}"

# ============================================================================
# PostgreSQL Database Configuration
# ============================================================================
[postgresql]
    # Database host address (from .env: POSTGRESQL_HOST)
    host = "${POSTGRESQL_HOST}"
    
    # Database port (from .env: POSTGRESQL_PORT)
    port = "${POSTGRESQL_PORT}"
    
    # Database user (from .env: POSTGRESQL_USER)
    user = "${POSTGRESQL_USER}"
    
    # Database password (from .env: POSTGRESQL_PASSWORD)
    password = "${POSTGRESQL_PASSWORD}"
    
    # Database name (from .env: POSTGRESQL_NAME_DEV or POSTGRESQL_NAME_PROD)
    # Use POSTGRESQL_NAME_DEV for dev.toml, POSTGRESQL_NAME_PROD for prod.toml
    dbname = "${POSTGRESQL_NAME_<ENV>}"
    
    # SSL mode: "disable" | "require" | "verify-ca" | "verify-full"
    # Use "disable" for development, "require" for production
    sslmode = "<disable|require>"
    
    # Timezone (use "UTC" for consistency)
    timezone = "UTC"
    
    # Logger level: "silent" | "error" | "warn" | "info"
    # Use "warn" for development, "error" for production
    logger_level = "<warn|error>"
    
    # Auto migrate database schema on startup (use false in production)
    auto_migrate = false

    # Connection pool configuration
    [postgresql.connection]
        # Connection timeout (e.g., "5s", "10s")
        timeout = "5s"
        
        # Maximum number of retry attempts
        max_retries = 5
        
        # Retry interval between attempts (e.g., "2s")
        retry_interval = "2s"
        
        # Maximum number of idle connections in the pool
        max_idle_connections = 10
        
        # Maximum number of open connections to the database
        max_open_connections = 100
        
        # Maximum amount of time a connection may be idle (e.g., "15m")
        conn_max_idle_time = "15m"
        
        # Maximum amount of time a connection may be reused (e.g., "1h")
        conn_max_lifetime = "1h"

# ============================================================================
# Redis Cache Configuration
# ============================================================================
[cache]
    # Redis host address (from .env: REDIS_HOST)
    host = "${REDIS_HOST}"
    
    # Redis port (from .env: REDIS_PORT)
    port = "${REDIS_PORT}"
    
    # Redis password (from .env: REDIS_PASSWORD)
    password = "${REDIS_PASSWORD}"

    # Cache connection settings
    [cache.connection]
        # Dial timeout (e.g., "3s")
        dial_timeout = "3s"
        
        # Write timeout (e.g., "3s")
        write_timeout = "3s"
        
        # Read timeout (e.g., "3s")
        read_timeout = "3s"
        
        # Maximum number of retry attempts
        max_retries = 5
        
        # Retry interval between attempts (e.g., "2s")
        retry_interval = "2s"

    # TLS configuration for Redis
    [cache.tls]
        # Enable TLS connection (use true for secure connections)
        enabled = false
        
        # Server name for TLS verification
        server_name = ""

# ============================================================================
# Logger Configuration
# ============================================================================
[logger]
    # Log level: "debug" | "info" | "warn" | "error"
    # Use "debug" for development, "info" for production
    level = "<debug|info>"
    
    # Log format: "json" | "console"
    # Use "console" for development, "json" for production
    format = "<console|json>"
    
    # Output destination: "stdout" | "file"
    output = "stdout"
    
    # Log file path (used when output = "file")
    file_path = "logs/<service>.log"
    
    # Maximum log file size in MB before rotation
    max_size_mb = 100
    
    # Maximum number of backup log files to keep
    max_backups = 10
    
    # Maximum age of log files in days before deletion
    max_age_day = 30
    
    # Compress rotated log files
    compress = false

# ============================================================================
# Token Configuration (Required for: auth, clients services)
# ============================================================================
[token]
    # Secret key for JWT token signing (from .env: TOKEN_SECRET_KEY)
    secret_key = "${TOKEN_SECRET_KEY}"
    
    # Token issuer name (from .env: TOKEN_ISSUER)
    issuer = "${TOKEN_ISSUER}"
    
    # Access token time-to-live (from .env: TOKEN_ACCESS_TTL)
    access_token_ttl = "${TOKEN_ACCESS_TTL}"
    
    # Refresh token time-to-live (from .env: TOKEN_REFRESH_TTL)
    refresh_token_ttl = "${TOKEN_REFRESH_TTL}"
    
    # JWT signing algorithm (from .env: TOKEN_ALGORITHM)
    algorithm = "${TOKEN_ALGORITHM}"

# ============================================================================
# Email Configuration (Required for: auth service)
# ============================================================================
[email]
    # SMTP server host (from .env: EMAIL_SMTP_HOST)
    smtp_host = "${EMAIL_SMTP_HOST}"
    
    # SMTP server port (from .env: EMAIL_SMTP_PORT)
    smtp_port = "${EMAIL_SMTP_PORT}"
    
    # SMTP username (from .env: EMAIL_SMTP_USER)
    smtp_user = "${EMAIL_SMTP_USER}"
    
    # SMTP password (from .env: EMAIL_SMTP_PASSWORD)
    smtp_password = "${EMAIL_SMTP_PASSWORD}"
    
    # From email address (from .env: EMAIL_SMTP_FROM)
    smtp_from = "${EMAIL_SMTP_FROM}"

# ============================================================================
# Storage Configuration (Required for: image service)
# ============================================================================
[storage]
    # Base path for file storage
    base_path = "./data"

# ============================================================================
# Kafka Message Broker Configuration
# ============================================================================
[kafka]
    # Kafka broker addresses (from .env: KAFKA_BROKERS)
    # Format: "host:port" (e.g., "192.168.1.64:10183")
    brokers = ["${KAFKA_BROKERS}"]
    
    # Client ID for Kafka connections
    client_id = "nfxid-<module>-service"

    # Network configuration
    [kafka.network]
        # Maximum number of open requests per connection
        max_open_requests = 1

    # Producer configuration
    [kafka.producer]
        # Acknowledgment mode: "0" (no ack) | "1" (leader only) | "all" (all replicas)
        acks = "all"
        
        # Compression type: "none" | "gzip" | "snappy" | "lz4" | "zstd"
        compression = "snappy"
        
        # Number of retries for failed messages
        retries = 3
        
        # Batch size in bytes (1MB = 1048576)
        batch_bytes = 1048576
        
        # Delay in milliseconds before sending a batch
        linger_ms = 5
        
        # Enable idempotent producer (prevents duplicate messages)
        idempotent = true

    # Consumer configuration
    [kafka.consumer]
        # Consumer group ID (must be unique per service)
        group_id = "<module>-service-group"
        
        # Initial offset: "earliest" (from beginning) | "latest" (from end)
        initial_offset = "latest"
        
        # Session timeout in milliseconds
        session_timeout_ms = 10000
        
        # Heartbeat interval in milliseconds
        heartbeat_interval_ms = 3000
        
        # Minimum bytes to fetch per request
        fetch_min_bytes = 1
        
        # Maximum bytes to fetch per request (5MB = 5242880)
        fetch_max_bytes = 5242880
        
        # Return errors to consumer
        return_errors = true

    # Security configuration
    [kafka.security]
        # Enable SASL authentication
        enabled = false
        
        # SASL mechanism: "PLAIN" | "SCRAM-SHA-256" | "SCRAM-SHA-512"
        mechanism = "PLAIN"
        
        # SASL username
        username = ""
        
        # SASL password
        password = ""
        
        # Skip TLS certificate verification (use false in production)
        tls_insecure_skip_verify = false

    # Producer topics: map message keys to topic names
    # Format: message_key = "topic-name"
    [kafka.producer_topics]
        # Example:
        # <module> = "nfxid.<module>"
        # <module>_poison = "nfxid.<module>_poison"

    # Consumer topics: map message keys to topic names
    # Format: message_key = "topic-name"
    [kafka.consumer_topics]
        # Example:
        # <module> = "nfxid.<module>"
        # <module>_poison = "nfxid.<module>_poison"

# ============================================================================
# RabbitMQ Message Broker Configuration
# ============================================================================
[rabbitmq]
    # Connection configuration
    # All values from .env: RABBITMQ_HOST, RABBITMQ_PORT, RABBITMQ_USER, RABBITMQ_PASSWORD, RABBITMQ_VHOST
    host = "${RABBITMQ_HOST}"
    port = "${RABBITMQ_PORT}"
    user = "${RABBITMQ_USER}"
    password = "${RABBITMQ_PASSWORD}"
    vhost = "${RABBITMQ_VHOST}"
    
    # Client ID for RabbitMQ connections
    client_id = "nfxid-<module>-service"

    # Producer configuration
    [rabbitmq.producer]
        # If true, return error when message cannot be routed
        mandatory = false
        
        # If true, return error when message cannot be delivered immediately
        immediate = false
        
        # Message content type
        content_type = "application/json"
        
        # Delivery mode: 1 = non-persistent, 2 = persistent
        delivery_mode = 2
        
        # Wait for server confirmation (Publisher Confirms)
        confirm_delivery = true
        
        # Channel pool size (0 = no pooling)
        channel_pool_size = 10
        
        # Enable transactional mode
        transactional = false
        
        # Default message priority (0-255, 0 = no priority)
        default_priority = 0

    # Consumer configuration
    [rabbitmq.consumer]
        # Queue name (if empty, auto-generated from topic)
        queue_name = ""
        
        # Consumer tag identifier
        consumer_tag = "nfxid-<module>-consumer"
        
        # Automatically acknowledge messages (not recommended)
        auto_ack = false
        
        # Exclusive queue (only this consumer can access)
        exclusive = false
        
        # Don't receive messages published on the same connection
        no_local = false
        
        # Don't wait for server response
        no_wait = false
        
        # Don't requeue message on nack
        no_requeue_on_nack = false
        
        # Prefetch count (0 = unlimited)
        prefetch_count = 10
        
        # Prefetch size in bytes (0 = unlimited)
        prefetch_size = 0
        
        # Apply QOS globally to all channels
        qos_global = false

    # Connection settings
    [rabbitmq.connection]
        # TLS configuration
        [rabbitmq.connection.tls]
            # Enable TLS connection
            enabled = false
            
            # Skip TLS certificate verification (use false in production)
            insecure_skip_verify = false
            
            # Client certificate file path
            cert_file = ""
            
            # Client private key file path
            key_file = ""
            
            # CA certificate file path
            ca_file = ""
            
            # Server name for TLS verification
            server_name = ""

        # Reconnection settings
        [rabbitmq.connection.reconnect]
            # Enable automatic reconnection
            enabled = true
            
            # Initial reconnection interval (e.g., "1s")
            initial_interval = "1s"
            
            # Randomization factor for backoff (0.0 - 1.0)
            randomization_factor = 0.5
            
            # Backoff multiplier
            multiplier = 1.5
            
            # Maximum reconnection interval (e.g., "30s")
            max_interval = "30s"

        # AMQP protocol settings
        [rabbitmq.connection.amqp]
            # Virtual host (from .env: RABBITMQ_VHOST)
            vhost = "${RABBITMQ_VHOST}"
            
            # Heartbeat interval in seconds (0 = disabled)
            heartbeat = 10
            
            # Locale setting
            locale = "en_US"
            
            # Maximum number of channels (0 = unlimited)
            channel_max = 0
            
            # Maximum frame size (0 = use default)
            frame_size = 0

    # Exchange configuration
    [rabbitmq.exchange]
        # Exchange name (if empty, auto-generated from topic)
        name = ""
        
        # Exchange type: "direct" | "topic" | "fanout" | "headers"
        # Plugin types: "x-delayed-message" | "x-consistent-hash" | etc.
        type = "topic"
        
        # Exchange survives broker restart
        durable = true
        
        # Auto-delete when no queues are bound
        auto_delete = false
        
        # Internal exchange (cannot be published to directly)
        internal = false
        
        # Don't wait for server response
        no_wait = false

    # Queue configuration
    [rabbitmq.queue]
        # Queue survives broker restart
        durable = true
        
        # Auto-delete when no consumers
        auto_delete = false
        
        # Exclusive queue (only this connection can access)
        exclusive = false
        
        # Don't wait for server response
        no_wait = false
        
        # Maximum priority (0-255, 0 = priority queue disabled)
        max_priority = 0
        
        # Message TTL in milliseconds (0 = no TTL)
        message_ttl = 0
        
        # Maximum queue length in messages (0 = unlimited)
        max_length = 0
        
        # Maximum queue length in bytes (0 = unlimited)
        max_length_bytes = 0
        
        # Dead letter exchange name
        dead_letter_exchange = ""
        
        # Dead letter routing key
        dead_letter_routing_key = ""

    # Queue binding configuration
    [rabbitmq.queue_bind]
        # Routing key (if empty, uses topic name)
        routing_key = ""
        
        # Don't wait for server response
        no_wait = false

    # Producer exchanges: map message keys to exchange and routing key
    # Format: message_key = {exchange = "exchange_name", routing_key = "routing.key"}
    # - If exchange is empty, uses exchange.name config or auto-generated
    # - If routing_key is empty, uses message key as routing_key
    [rabbitmq.producer_exchanges]
        # Example:
        # event = {exchange = "", routing_key = "<module>.event"}
        # event_poison = {exchange = "", routing_key = "<module>.event.poison"}

    # Consumer queues: map message keys to queue and binding key
    # Format: message_key = {queue = "queue_name", binding_key = "binding.key"}
    # - If queue is empty, uses consumer.queue_name config or auto-generated
    # - If binding_key is empty, uses message key as binding_key
    [rabbitmq.consumer_queues]
        # Example:
        # event = {queue = "", binding_key = "<module>.event"}
        # event_poison = {queue = "", binding_key = "<module>.event.poison"}

# ============================================================================
# gRPC Client Configuration
# ============================================================================
# Inter-service communication addresses
# All addresses use environment variables from .env:
#   GRPC_HOST_<MODULE>: Service host (IP address or Docker service name)
#   GRPC_PORT_<MODULE>: Service port
[grpc_client]
    # Service addresses - format: "${GRPC_HOST_<MODULE>}:${GRPC_PORT_<MODULE>}"
    # Each service can be deployed on a different host (supports multi-host deployment)
    
    # Access service gRPC address
    access_addr = "${GRPC_HOST_ACCESS}:${GRPC_PORT_ACCESS}"
    
    # Auth service gRPC address
    auth_addr = "${GRPC_HOST_AUTH}:${GRPC_PORT_AUTH}"
    
    # Audit service gRPC address
    audit_addr = "${GRPC_HOST_AUDIT}:${GRPC_PORT_AUDIT}"
    
    # Clients service gRPC address
    clients_addr = "${GRPC_HOST_CLIENTS}:${GRPC_PORT_CLIENTS}"
    
    # Directory service gRPC address
    directory_addr = "${GRPC_HOST_DIRECTORY}:${GRPC_PORT_DIRECTORY}"
    
    # Image service gRPC address
    image_addr = "${GRPC_HOST_IMAGE}:${GRPC_PORT_IMAGE}"
    
    # System service gRPC address
    system_addr = "${GRPC_HOST_SYSTEM}:${GRPC_PORT_SYSTEM}"
    
    # Tenants service gRPC address
    tenants_addr = "${GRPC_HOST_TENANTS}:${GRPC_PORT_TENANTS}"

# ============================================================================
# Service-Specific Configuration Notes
# ============================================================================
# 
# 1. Required sections for ALL services:
#    - [server]
#    - [postgresql]
#    - [cache]
#    - [logger]
#    - [kafka] OR [rabbitmq] (choose one message broker)
#    - [grpc_client]
#
# 2. Optional sections by service:
#    - [token]: Required for auth, clients services
#    - [email]: Required for auth service
#    - [storage]: Required for image service
#
# 3. Environment Variable Substitution:
#    - All placeholders in the format "${VAR_NAME}" will be replaced at runtime
#    - Values are loaded from the .env file in the project root
#    - If a variable is missing, the application will exit with an error
#    - See .example.env for all available environment variables
#
# 4. Service port ranges (gRPC):
#    - access:   10155 (GRPC_PORT_ACCESS)
#    - auth:     10156 (GRPC_PORT_AUTH)
#    - audit:    10157 (GRPC_PORT_AUDIT)
#    - clients:  10158 (GRPC_PORT_CLIENTS)
#    - directory: 10159 (GRPC_PORT_DIRECTORY)
#    - image:    10160 (GRPC_PORT_IMAGE)
#    - system:   10161 (GRPC_PORT_SYSTEM)
#    - tenants:  10162 (GRPC_PORT_TENANTS)
#
# 5. Multi-host deployment support:
#    - Each service can be configured with a different host IP using GRPC_HOST_<MODULE>
#    - For single-host deployment, set all GRPC_HOST_* to the same IP
#    - For multi-host deployment, set each GRPC_HOST_* to the respective host IP
#
# 6. Kafka topics should be configured in [kafka.producer_topics] and
#    [kafka.consumer_topics] according to your service's event requirements.
#
# 7. RabbitMQ exchanges/queues should be configured in 
#    [rabbitmq.producer_exchanges] and [rabbitmq.consumer_queues] according 
#    to your service's event requirements.
#
# 8. Development vs Production differences:
#    - dev.toml: Use POSTGRESQL_NAME_DEV, sslmode="disable", logger_level="warn", compress=false
#    - prod.toml: Use POSTGRESQL_NAME_PROD, sslmode="require", logger_level="error", compress=true
#
# ============================================================================
